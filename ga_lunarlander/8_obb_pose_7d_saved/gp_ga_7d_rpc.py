#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import argparse
import random
import multiprocessing as mp
from multiprocessing.connection import Client
from typing import Optional, Tuple
import json
from datetime import datetime

import numpy as np
import torch
import torch.nn as nn
import gymnasium as gym
# [FIX] ç¡®ä¿å¯¼å…¥ cv2
import cv2
from tqdm import tqdm

# [PLOT] ç¦»å±ç»˜å›¾
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

# ä¸ºå¤šè¿›ç¨‹è®¾ç½® PyTorch çº¿ç¨‹æ•°ï¼Œé¿å…èµ„æºç«äº‰
os.environ.setdefault("OMP_NUM_THREADS", "1")
os.environ.setdefault("MKL_NUM_THREADS", "1")
os.environ.setdefault("KMP_DUPLICATE_LIB_OK", "TRUE")

# è®© tqdm è¾“å‡ºæ›´å¹²å‡€
print = lambda *a, **k: (__import__("builtins").print(*a, **{**k, "flush": True}))


# ----------------------------
# ç®€å• 2å±‚ MLP: è¾“å…¥ 7D -> è¾“å‡º 4 åŠ¨ä½œ logits
# ----------------------------
class NNPolicy(nn.Module):
    def __init__(self, hidden=64):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(7, hidden), nn.ReLU(),
            nn.Linear(hidden, 4)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.net(x)

def get_weights_vector(m: nn.Module) -> np.ndarray:
    with torch.no_grad():
        return torch.cat([p.data.flatten() for p in m.parameters()]).cpu().numpy()

def set_weights_vector(m: nn.Module, vec: np.ndarray):
    offset = 0
    with torch.no_grad():
        for p in m.parameters():
            n = p.numel()
            p.data.copy_(torch.from_numpy(vec[offset:offset+n]).view_as(p))
            offset += n

def uniform_crossover(p1: np.ndarray, p2: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    mask = np.random.rand(len(p1)) < 0.5
    c1 = np.where(mask, p1, p2)
    c2 = np.where(~mask, p1, p2)
    return c1, c2

def mutate(vec: np.ndarray, sigma: float) -> np.ndarray:
    return vec + np.random.randn(vec.size) * sigma

# ----------------------------
# RPC Client for 7D state
# ----------------------------
class RPCClient:
    def __init__(self, host: str, port: int, authkey: str = "yolo-rpc"):
        self.address = (host, port)
        self.authkey = authkey.encode("utf-8")
        self.conn: Optional[Client] = None

    def __enter__(self):
        try:
            self.conn = Client(self.address, authkey=self.authkey)
        except ConnectionRefusedError:
            print(f"[FATAL] Connection to RPC server {self.address} refused. Is the server running?")
            # æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ä¸Šå±‚æ•è·
            raise
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.conn:
            self.conn.close()

    def reset(self):
        self.conn.send(("reset", None))
        _ = self.conn.recv()

    # [FIX] å°†å‚æ•°åæ”¹ä¸º stepsï¼Œæ›´æ¸…æ™°
    def update_full(self, frame_bgr: np.ndarray, steps: int = 0):
        self.conn.send(("update_full", (frame_bgr, steps)))
        ok, z = self.conn.recv()
        return z if ok else None

    def predict_only(self):
        self.conn.send(("predict_only", None))
        ok, z = self.conn.recv()
        return z if ok else None

# ----------------------------
# ä¸ªä½“è¯„ä¼°
# ----------------------------
def evaluate_individual(args):
    idx, weights, rpc_host, rpc_port, authkey = args
    model = NNPolicy()
    set_weights_vector(model, weights)

    total_reward = 0.0
    try:
        env = gym.make("LunarLander-v3", render_mode="rgb_array")
        obs, _ = env.reset(seed=random.randint(0, 9999))
        
        with RPCClient(rpc_host, rpc_port, authkey) as rpc:
            rpc.reset()

            for t in range(1000): # t æ˜¯å½“å‰çš„æ­¥æ•°
                frame = env.render()
                if frame is None: break
                
                # RGB -> BGR
                frame_bgr = frame[..., ::-1]

                # [FIX] å°†å½“å‰æ­¥æ•° t ä¼ é€’ç»™æœåŠ¡å™¨
                state = rpc.update_full(frame_bgr, t)
                
                # å¦‚æœæ¨ç†å¤±è´¥ï¼Œå°è¯•ç”¨è¿åŠ¨æ¨¡å‹é¢„æµ‹ï¼›å¦‚æœè¿˜å¤±è´¥ï¼Œåˆ™ç”¨0å‘é‡å…œåº•
                if state is None:
                    state = rpc.predict_only()
                if state is None:
                    state = np.zeros(7, dtype=np.float32)

                s = torch.tensor(state, dtype=torch.float32)
                with torch.no_grad():
                    act = int(torch.argmax(model(s)).item())

                obs, reward, done, truncated, info = env.step(act)
                total_reward += reward

                if done or truncated:
                    break
        
        env.close()
        return idx, total_reward

    except Exception as e:
        # å¯é€‰ debug:
        # print(f"[Worker Error] Process {idx} failed: {e}")
        return idx, -1000.0 # è¿”å›ä¸€ä¸ªæä½çš„åˆ†æ•°è¡¨ç¤ºå¤±è´¥


# ----------------------------
# [SAVE] å·¥å…·å‡½æ•°ï¼šä¿å­˜æ¨¡å‹ã€æ›²çº¿ä¸æŒ‡æ ‡
# ----------------------------
def save_best_model(run_dir: str, vec: np.ndarray):
    """ä¿å­˜å†å²æœ€ä¼˜ä¸ªä½“ï¼šå‘é‡ + state_dict"""
    os.makedirs(run_dir, exist_ok=True)
    # å‘é‡
    np.savez(os.path.join(run_dir, "best_model_vec.npz"), weights=vec)
    # state_dict
    m = NNPolicy()
    set_weights_vector(m, vec)
    torch.save(m.state_dict(), os.path.join(run_dir, "best_model.pt"))

def save_metrics_csv(run_dir: str, gens, best_list, avg_list):
    path = os.path.join(run_dir, "metrics.csv")
    header_needed = not os.path.exists(path)
    with open(path, "a", encoding="utf-8") as f:
        if header_needed:
            f.write("generation,best,avg_top\n")
        for g, b, a in zip(gens, best_list, avg_list):
            f.write(f"{g},{b},{a}\n")

def plot_reward_curve(run_dir: str, all_gens, all_best, all_avg):
    """ä¿å­˜å¥–åŠ±æ›²çº¿å›¾ï¼šbest vs avg_top"""
    if len(all_gens) == 0:
        return
    plt.figure(figsize=(8,5))
    plt.plot(all_gens, all_best, label="Best reward")
    plt.plot(all_gens, all_avg, label="AvgTop")
    plt.xlabel("Generation")
    plt.ylabel("Reward")
    plt.title("GA Reward Curve")
    plt.grid(True, linestyle="--", alpha=0.4)
    plt.legend()
    out = os.path.join(run_dir, "reward_curve.png")
    plt.tight_layout()
    plt.savefig(out, dpi=150)
    plt.close()

# ----------------------------
# GA ä¸»å¾ªç¯
# ----------------------------
def run_ga(population_size, generations, elite_frac, sigma, processes,
           rpc_host, rpc_port, authkey,
           outdir: str, tag: Optional[str] = None):

    # [LOG] è¿è¡Œç›®å½•
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_name = f"ga_{ts}" if not tag else f"ga_{ts}_{tag}"
    run_dir = os.path.join(outdir, run_name)
    os.makedirs(run_dir, exist_ok=True)

    # [LOG] ä¿å­˜é…ç½®
    cfg = {
        "population": population_size,
        "generations": generations,
        "elite_frac": elite_frac,
        "init_sigma": sigma,
        "processes": processes,
        "rpc_host": rpc_host,
        "rpc_port": rpc_port,
        "authkey": authkey,
        "outdir": outdir,
        "run_dir": run_dir,
        "tag": tag or ""
    }
    with open(os.path.join(run_dir, "config.json"), "w", encoding="utf-8") as f:
        json.dump(cfg, f, indent=2, ensure_ascii=False)

    # åˆå§‹åŒ–
    model = NNPolicy()
    base_vec = get_weights_vector(model)
    pop = [mutate(base_vec, sigma=0.2) for _ in range(population_size)]

    # [LOG] å†å²æœ€ä½³è¿½è¸ª & æ›²çº¿ç¼“å­˜
    best_global = -np.inf
    best_vec_global = None
    curve_gens, curve_best, curve_avg = [], [], []

    for gen in range(1, generations + 1):
        jobs = [(i, pop[i], rpc_host, rpc_port, authkey) for i in range(population_size)]
        
        scores = np.zeros(population_size)
        try:
            with mp.Pool(processes=processes) as pool:
                results = list(tqdm(pool.imap_unordered(evaluate_individual, jobs),
                                    total=len(jobs),
                                    desc=f"Generation {gen}/{generations}"))
            for idx, r in results:
                scores[idx] = r
        except Exception as e:
            print(f"\n[FATAL] Multiprocessing pool failed: {e}")
            print("Please ensure the RPC server is running and accessible.")
            # [SAVE] å³ä½¿å¤±è´¥ä¹ŸæŠŠå·²æœ‰æ›²çº¿ä¸æŒ‡æ ‡ä¿å­˜
            plot_reward_curve(run_dir, curve_gens, curve_best, curve_avg)
            # åªæŠŠæ–°ä¸€ä»£çš„ç»Ÿè®¡è½ä¸€ä¸‹ï¼ˆå¦‚æœè¿˜æ²¡åšï¼‰
            break

        elite_num = max(2, int(elite_frac * population_size))
        elite_idx = np.argsort(scores)[-elite_num:]
        elites = [pop[i] for i in elite_idx]

        best = float(scores[elite_idx[-1]])
        avg_top = float(scores[elite_idx].mean())
        print(f"ğŸ† [GEN {gen:03d}] Best={best:+.2f}  AvgTop{elite_num}={avg_top:+.2f}  Sigma={sigma:.3f}")

        # [LOG] æ›²çº¿ç´¯ç§¯å¹¶è½ç›˜
        curve_gens.append(gen)
        curve_best.append(best)
        curve_avg.append(avg_top)
        # é€ä»£è¦†ç›–ä¿å­˜ metrics.csvï¼ˆåªè¿½åŠ æœ¬ä»£ï¼›åŒæ—¶æ¯æ¬¡é‡ç”»æ›²çº¿ä¿è¯ä¸­é€”ä¸­æ–­ä¹Ÿæœ‰å›¾ï¼‰
        save_metrics_csv(run_dir, [gen], [best], [avg_top])
        plot_reward_curve(run_dir, curve_gens, curve_best, curve_avg)

        # [SAVE] å†å²æœ€å¥½ï¼Œç«‹å³ä¿å­˜æ¨¡å‹ï¼ˆå‘é‡ + state_dictï¼‰
        if best > best_global:
            best_global = best
            best_vec_global = pop[int(elite_idx[-1])]
            save_best_model(run_dir, best_vec_global)

        # ç¹è¡
        new_pop = elites.copy()
        while len(new_pop) < population_size:
            p1, p2 = random.sample(elites, 2)
            c1, c2 = uniform_crossover(p1, p2)
            new_pop.append(mutate(c1, sigma))
            if len(new_pop) < population_size:
                new_pop.append(mutate(c2, sigma))

        pop = new_pop
        sigma = max(0.01, sigma * 0.99)  # é€€ç«

    # [SAVE] è®­ç»ƒæ­£å¸¸ç»“æŸåï¼Œç¡®ä¿æœ€ç»ˆæ›²çº¿ä¸ best æ¨¡å‹å­˜åœ¨
    plot_reward_curve(run_dir, curve_gens, curve_best, curve_avg)
    if best_vec_global is not None:
        save_best_model(run_dir, best_vec_global)


# ----------------------------
# CLI
# ----------------------------
def main():
    parser = argparse.ArgumentParser(description="GA Client for LunarLander with RPC state estimation")
    parser.add_argument("--rpc-host", type=str, default="127.0.0.1", help="RPC server host")
    parser.add_argument("--rpc-port", type=int, default=6001, help="RPC server port")
    parser.add_argument("--authkey", type=str, default="yolo-rpc", help="RPC authentication key")
    parser.add_argument("--processes", type=int, default=8, help="Number of parallel processes")
    parser.add_argument("--population", type=int, default=120, help="Population size")
    parser.add_argument("--generations", type=int, default=100, help="Number of generations")
    parser.add_argument("--elite-frac", type=float, default=0.1, help="Fraction of elites to keep")
    parser.add_argument("--sigma", type=float, default=0.1, help="Initial mutation strength")

    # [LOG] æ–°å¢è¾“å‡ºç›®å½•ä¸å¯é€‰ tag
    parser.add_argument("--outdir", type=str, default="runs_ga", help="Directory to save logs/models")
    parser.add_argument("--tag", type=str, default="", help="Optional tag to append to run folder name")

    args = parser.parse_args()

    print(f"ğŸš€ [INFO] Starting GA training with {args.processes} processes, "
          f"pop={args.population}, gen={args.generations}")
    print(f"ğŸ“¡ Connecting to RPC server at {args.rpc_host}:{args.rpc_port}")

    run_ga(args.population, args.generations, args.elite_frac, args.sigma,
           args.processes, args.rpc_host, args.rpc_port, args.authkey,
           outdir=args.outdir, tag=(args.tag or None))

if __name__ == "__main__":
    main()
